# -*- coding: utf-8 -*-
"""Copy of  NLP project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MU-5-PXCSUSL6Q4ZRi-Jxigcn2NUioS8

<div dir="rtl">

×‘×©×œ×‘ ×–×” ×‘× ×™× ×• ×ª×”×œ×™×š ×œ××™×¡×•×£, × ×™×§×•×™ ×•×©××™×¨×ª ×”× ×ª×•× ×™× ×‘×¦×•×¨×” × ×•×—×” ×œ×©×™××•×© ×—×•×–×¨.  
×ª×—×™×œ×” ×”×¢×œ×™× ×• ×œ-Colab ×§×•×‘×¥ ZIP ×¢× ×“×¤×™ ×ª×•×›×Ÿ ×××ª×¨ JCT, ×—×™×œ×¦× ×• ××•×ª×• ×•×§×¨×× ×• ××ª ×§×•×‘×¥ ×”-JSON ×©×‘×ª×•×›×• ×›×“×™ ×œ××¡×•×£ ××ª ×›×œ ×›×ª×•×‘×•×ª ×”-URL ×”×™×™×—×•×“×™×•×ª.  
×œ××—×¨ ××›×Ÿ ×‘×™×¦×¢× ×• ×¡×¨×™×§×” (scraping) ×©×œ ×›×œ ××—×“ ××”×“×¤×™×: ×”×•×¨×“× ×• ××ª ×”×ª×•×›×Ÿ ×©×œ×”×, × ×™×§×™× ×• ××•×ª×• ××§×•×“ HTML, ×¡×§×¨×™×¤×˜×™× ×•×¢×™×¦×•×‘×™× ×•×”×©××¨× ×• ×˜×§×¡×˜ × ×§×™ ×‘×œ×‘×“.  
××ª ×›×œ ×”××™×“×¢ â€” ×›×ª×•×‘×ª ×•×ª×•×›×Ÿ × ×§×™ â€” ×©××¨× ×• ×‘×§×•×‘×¥ ×—×“×© ×‘×©× `lev_scraped_with_content.json`, ×›×š ×©×‘×¤×¢××™× ×”×‘××•×ª × ×•×›×œ ×œ×˜×¢×•×Ÿ ××ª ×”×§×•×‘×¥ ×”×–×” ×™×©×™×¨×•×ª ×•×œ× × ×¦×˜×¨×š ×œ×‘×¦×¢ ×©×•×‘ ××ª ×”×¡×¨×™×§×”.  
×‘×›×š ×”×›× ×• ×œ×¢×¦×× ×• ×‘×¡×™×¡ × ×ª×•× ×™× × ×§×™ ×•××•×›×Ÿ ×œ××™××•×Ÿ ××• ××™× ×“×•×§×¡.

</div>
"""

!pip install sentence-transformers faiss-cpu transformers accelerate

!pip install faiss-cpu

import json, re
import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

from google.colab import drive
drive.mount('/content/drive')

# ×˜×¢×Ÿ ××ª ×”×“××˜×”
#encoding='utf-8' × ×•×¢×“ ×œ×•×•×“× ×©×”×ª×•×•×™× ×‘×¢×‘×¨×™×ª ××• ×ª×•×•×™× ××™×•×—×“×™× ×œ× ×™×™×¤×’×¢×•.


with open('/content/drive/MyDrive/jct_scraped_pages.json', encoding='utf-8') as f:
    data = json.load(f)
#××•×¡×™×¤×” ×¤×” ×“×¨×š ×œ×”××™×¨ × ×›×•×Ÿ ××ª ×”×§×•×‘×¥ ×œDF
import json
import pandas as pd



# ×”××¨ ×œ××‘× ×” × ×›×•×Ÿ ×œ×“××˜×”×¤×¨×™×™×
rows = []
for url, info in data.items():
    rows.append({
        "url": url,
        "title": info.get("title", ""),
        "content": info.get("content", "")
    })

df = pd.DataFrame(rows)

#×œ×‘×“×•×§ ×©×‘×××ª ×™×© ×˜×‘×œ×” ××—×¨×ª ××™×Ÿ ××” ×œ×”××©×™×š ×™×”×”×™ ×©×’××•×ª ×ª×™×§×•×Ÿ ×œ×§×•×“
df.head()

print(df.columns)

# × ×§×” ×˜×§×¡×˜
#×‘×©×‘×™×œ ×”×›× ×” ×œ EMBEDING
def clean_text(text):
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text
#×™×©×•× ×”× ×™×§×•×™
df['clean_content'] = df['content'].apply(clean_text)
# ×©×™×¤×•×¨ × ×™×§×•×™:
df['clean_content'] = df['clean_content'].str.replace(r'http\S+', '', regex=True)
df['clean_content'] = df['clean_content'].str.replace(r'[^\w\s×-×ª]', '', regex=True)

# ×¡×™× ×•×Ÿ ×˜×§×¡×˜×™× ×§×¦×¨×™× ××• ×¨×™×§×™×:
df = df[~df['clean_content'].str.strip().eq('')]
df['text_length'] = df['clean_content'].apply(len)
df = df[df['text_length'] > 100]


# ×›××” ×©×•×¨×•×ª?
print(f"âœ… ××¡×¤×¨ ×©×•×¨×•×ª ×‘×“××˜×”: {len(df)}")

# ××•×¨×š ×˜×§×¡×˜×™×
# ×œ×¡×™× ×•×Ÿ ×˜×§×¡×˜×™× ×§×¦×¨×™× ××“×™/××¨×•×›×™× ××“×™, ×•×œ× ×™×ª×•×—.
df['text_length'] = df['clean_content'].apply(len)
print("ğŸ“Š ×¡×˜×˜×™×¡×˜×™×§×•×ª ××•×¨×š ×”×˜×§×¡×˜×™×:")
print(df['text_length'].describe())

# ×“×•×’×××•×ª
#×××¤×©×¨ ×œ×¨××•×ª ×”×× ×”× ×™×§×•×™ ×¢×‘×“, ×•×”×× ×™×© ×˜×§×¡×˜×™× ×‘×¢×™×™×ª×™×™×.


for idx, row in df.sample(3).iterrows():
    print(f"\n--- ×˜×§×¡×˜ ×œ×“×•×’××” ({row['text_length']} ×ª×•×•×™×) ---")
    print(row['clean_content'][:1000], "...")

# ××—×•×– ×¤×¡×§××•×ª (newlines)
#×©×™××•×©×™ ×œ×“×¢×ª ×× ×”×˜×§×¡×˜×™× ××›×™×œ×™× ××‘× ×” ×©×œ ×¤×¡×§××•×ª ××• ×©×”× ×©×•×¨×” ××—×ª ××¨×•×›×”".
has_newlines = df['clean_content'].apply(lambda x: '\n' in x).mean()
print(f"\nğŸ“„ ××—×•×– ×˜×§×¡×˜×™× ×¢× ×¤×¡×§××•×ª (\\n): {has_newlines*100:.1f}%")

#× ×•×ª×Ÿ ××™× ×“×™×§×¦×™×” ×›××” ××”×˜×§×¡×˜×™× ×¨×œ×•×•× ×˜×™×™× ×œ× ×•×©× ××¡×•×™×.
keywords = df['clean_content'].str.contains(r'×‘×’×¨×•×ª|85|×ª× ××™ ×§×‘×œ×”', case=False, na=False).mean()
print(f"ğŸ“„ ××—×•×– ×˜×§×¡×˜×™× ×¢× ××™×œ×•×ª ××¤×ª×— ×¨×œ×•×•× ×˜×™×•×ª: {keywords*100:.1f}%")

import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize
import nltk
nltk.download('punkt_tab')

def smart_chunk(text, max_chars=384):
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = ""
    for sent in sentences:
        if len(current_chunk) + len(sent) <= max_chars:
            current_chunk += " " + sent
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sent
    if len(current_chunk.strip()) > 50:
     chunks.append(current_chunk.strip())


    return chunks

chunks = []
for idx, row in df.iterrows():
    for i, chunk in enumerate(smart_chunk(row['clean_content'])):
        chunks.append({
            'id': f"{idx}_{i}",
            'text': chunk,
            #××•×¡×™×¤×” ×¤×” ×©×™×“×¨×•×’
            'doc_id': idx

        })

chunks_df = pd.DataFrame(chunks)
print(f"âœ… × ×•×¦×¨×• {len(chunks_df)} ××§×˜×¢×™×.")
chunks_df.head()

from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
#××•×“×œ ×©××ª××™× ×’× ×œ×¢×‘×¨×™×ª
embed_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device='cuda')

embeddings = []
batch_size = 16
texts = chunks_df['text'].tolist()

for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    emb = embed_model.encode(batch, show_progress_bar=True)
    embeddings.append(emb)

embeddings = np.vstack(embeddings).astype("float32")
print(f"âœ… ×¡×™×™×× ×•, embeddings shape: {embeddings.shape}")

#  ×œ×•×•×“× ×©××™×Ÿ ××¤×¡×™×
print(np.isnan(embeddings).sum())
print(np.linalg.norm(embeddings, axis=1).min())


faiss.normalize_L2(embeddings)

index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings)

print(f"âœ… ××™× ×“×§×¡ ××•×›×Ÿ ×¢× {len(embeddings)} ××§×˜×¢×™×.")

print(chunks_df['text'].value_counts().head(10))

# ğŸ”· ×©××•×¨ embeddings
np.save('/content/drive/MyDrive/embeddings.npy', embeddings)
print("âœ… embeddings × ×©××¨×• ×‘×“×¨×™×™×‘")

# ğŸ”· ×©××•×¨ FAISS index
faiss.write_index(index, '/content/drive/MyDrive/faiss_index.index')
print("âœ… ××™× ×“×§×¡ FAISS × ×©××¨ ×‘×“×¨×™×™×‘")

# ğŸ”· ×©××•×¨ ××ª chunks_df ×›Ö¾CSV
chunks_df.to_csv('/content/drive/MyDrive/chunks_df.csv', index=False)
print("âœ… chunks_df × ×©××¨ ×‘×“×¨×™×™×‘")

def retrieve(query, k=5):
    query_emb = embed_model.encode([query]).astype("float32")
    faiss.normalize_L2(query_emb)
    D, I = index.search(query_emb, k)
    return I[0], D[0]

question = "What are the admission requirements?"
true_keywords = ["High School Diploma", "SAT", "Personal Interview"]

ids, scores = retrieve(question)

print("\nğŸ”· ×¤×¡×§××•×ª ×©× ×©×œ×¤×•:")
for idx, score in zip(ids, scores):
    text = chunks_df.iloc[idx]['text']
    print(f"\n--- (score: {score:.3f}) ---\n{text}")
    if any(kw.lower() in text.lower() for kw in true_keywords):
        print("âœ… ×¤×¡×§×” ×¨×œ×•×•× ×˜×™×ª × ××¦××” ×›××Ÿ!")

import pandas as pd

# ğŸ”· ×©××œ×•×ª ×•×ª×©×•×‘×•×ª × ×›×•× ×•×ª ×©×× ×™ ××¦×¤×”
questions = [
    {
        "question": "What are the admission requirements?",
        "expected_keywords": ["High School Diploma", "SAT", "Personal Interview"]
    },
    {
        "question": "How much is tuition?",
        "expected_keywords": ["$3,800", "tuition"]
    },
    {
        "question": "When does the semester start?",
        "expected_keywords": ["September", "school year", "first semester"]
    },
]

results = []

# ğŸ”· ×”×¤×•× ×§×¦×™×” ×©×œ× ×• ×œ××—×–×•×¨
def retrieve(query, k=5):
    query_clean = re.sub(r'[^\w\s]', '', query.lower())

    query_emb = embed_model.encode([query_clean]).astype("float32")
    faiss.normalize_L2(query_emb)
    D, I = index.search(query_emb, k)
    seen_texts = set()
    unique_indices = []
    unique_scores = []

    for idx, score in zip(I[0], D[0]):
        text = chunks_df.iloc[idx]['text']
        if text not in seen_texts:
            seen_texts.add(text)
            unique_indices.append(idx)
            unique_scores.append(score)
        if len(unique_indices) == 5:
            break

    return unique_indices, unique_scores


# ğŸ”· ×¢×‘×•×¨ ×›×œ ×©××œ×” â€” × ×‘×“×•×§ ×× ×”××™×“×¢ ×”×•×¤×™×¢
for q in questions:
    ids, scores = retrieve(q['question'], k=10)

    found = False
    retrieved_texts = []

    for idx, score in zip(ids, scores):
        text = chunks_df.iloc[idx]['text']
        retrieved_texts.append(text)
        if any(kw.lower() in text.lower() for kw in q['expected_keywords']):
            found = True

    results.append({
        "Question": q['question'],
        "Found?": "âœ…" if found else "âŒ",
        "Top Match": retrieved_texts[0][:150] + "...",
    })

# ğŸ”· ×˜×‘×œ×” ×™×¤×”
df_results = pd.DataFrame(results)
print(df_results)

# ğŸ”· ×“×™×•×§ ×›×•×œ×œ
accuracy = (df_results['Found?'] == "âœ…").mean()
print(f"\nğŸ¯ Recall@5: {accuracy*100:.1f}%")

!pip install transformers accelerate bitsandbytes

from huggingface_hub import login
login()  # â† ×ª×“×‘×™×§×• ××ª ×”×˜×•×§×Ÿ ×©×œ×›× ×›××Ÿ ×›×§×œ×˜

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from transformers import BitsAndBytesConfig

model_id = "mistralai/Mistral-7B-Instruct-v0.2"

# Configure 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # or "fp4"
    bnb_4bit_compute_dtype=torch.bfloat16,  # or torch.float16
    # bnb_4bit_use_double_quant=False, # optional
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    # torch_dtype="auto" # This might cause issues with 4-bit quantization
    quantization_config=bnb_config
)

qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)

def make_short_context(texts, max_chars=3500):
    context = ""
    for t in texts:
        if len(context) + len(t) > max_chars:
            break
        context += t.strip() + "\n"
    return context

def generate_answer(question, context, max_new_tokens=150):
    prompt = f"[INST] Context:\n{context}\n\nQuestion: {question}\nAnswer: [/INST]"
    output = qa_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0]['generated_text']


    # ×—×™×ª×•×š ×œ×¤×™ ×ª×’×™×•×ª INSTRUCT
    if "[/INST]" in output:
        return output.split("[/INST]")[-1].strip()
    else:
        return output.strip()

results = []

for idx, q in enumerate(questions):
    print(f"Processing question {idx+1}/{len(questions)}: {q['question'][:30]}...")  # ×”×“×¤×¡ ×”×ª×§×“××•×ª

    question = q["question"]
    expected_keywords = q["expected_keywords"]

    # ××—×–×•×¨ ×¤×¡×§××•×ª ×¨×œ×•×•× ×˜×™×•×ª
    ids, _ = retrieve(question)
    top_chunks = [chunks_df.iloc[i]['text'] for i in ids]
    context = make_short_context(top_chunks)

    # ×”×¤×§×ª ×ª×©×•×‘×”
    answer = generate_answer(question, context)

    # ×‘×“×™×§×ª ××™×œ×•×ª ××¤×ª×—
    found_keywords = [kw for kw in expected_keywords if kw.lower() in answer.lower()]
    found = "âœ…" if found_keywords else "âŒ"

    results.append({
        "Question": question,
        "Answer": answer,
        "Matched Keywords": ", ".join(found_keywords),
        "Success": found
    })

import pandas as pd

df_results = pd.DataFrame(results)
pd.set_option('display.max_colwidth', 200)
display(df_results)

accuracy = (df_results['Success'] == "âœ…").mean()
print(f"\nğŸ¯ ×“×™×•×§ ×›×•×œ×œ (Answer Accuracy): {accuracy*100:.1f}%")

# ×©××œ×” ×œ×“×•×’××”
question = "What are the admission requirements?"

# × ×©×œ×•×£ ××ª ×”×§×•× ×˜×§×¡×˜ ××”××—×–×•×¨ ×”×§×•×“×
ids, scores = retrieve(question)
context = "\n".join(chunks_df.iloc[i]['text'] for i in ids)

# ×™×•×¦×¨×™× ×ª×©×•×‘×”
generated = generate_answer(question, context)

print("ğŸ”· ×ª×©×•×‘×” ×©× ×•×¦×¨×”:")
print(generated)

!pip install -U bitsandbytes
