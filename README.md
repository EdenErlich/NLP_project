# -*- coding: utf-8 -*-
"""Copy of  NLP project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MU-5-PXCSUSL6Q4ZRi-Jxigcn2NUioS8

<div dir="rtl">

בשלב זה בנינו תהליך לאיסוף, ניקוי ושמירת הנתונים בצורה נוחה לשימוש חוזר.  
תחילה העלינו ל-Colab קובץ ZIP עם דפי תוכן מאתר JCT, חילצנו אותו וקראנו את קובץ ה-JSON שבתוכו כדי לאסוף את כל כתובות ה-URL הייחודיות.  
לאחר מכן ביצענו סריקה (scraping) של כל אחד מהדפים: הורדנו את התוכן שלהם, ניקינו אותו מקוד HTML, סקריפטים ועיצובים והשארנו טקסט נקי בלבד.  
את כל המידע — כתובת ותוכן נקי — שמרנו בקובץ חדש בשם `lev_scraped_with_content.json`, כך שבפעמים הבאות נוכל לטעון את הקובץ הזה ישירות ולא נצטרך לבצע שוב את הסריקה.  
בכך הכנו לעצמנו בסיס נתונים נקי ומוכן לאימון או אינדוקס.

</div>
"""

!pip install sentence-transformers faiss-cpu transformers accelerate

!pip install faiss-cpu

import json, re
import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

from google.colab import drive
drive.mount('/content/drive')

# טען את הדאטה
#encoding='utf-8' נועד לוודא שהתווים בעברית או תווים מיוחדים לא ייפגעו.


with open('/content/drive/MyDrive/jct_scraped_pages.json', encoding='utf-8') as f:
    data = json.load(f)
#מוסיפה פה דרך להמיר נכון את הקובץ לDF
import json
import pandas as pd



# המר למבנה נכון לדאטהפריים
rows = []
for url, info in data.items():
    rows.append({
        "url": url,
        "title": info.get("title", ""),
        "content": info.get("content", "")
    })

df = pd.DataFrame(rows)

#לבדוק שבאמת יש טבלה אחרת אין מה להמשיך יההי שגאות תיקון לקוד
df.head()

print(df.columns)

# נקה טקסט
#בשביל הכנה ל EMBEDING
def clean_text(text):
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text
#ישום הניקוי
df['clean_content'] = df['content'].apply(clean_text)
# שיפור ניקוי:
df['clean_content'] = df['clean_content'].str.replace(r'http\S+', '', regex=True)
df['clean_content'] = df['clean_content'].str.replace(r'[^\w\sא-ת]', '', regex=True)

# סינון טקסטים קצרים או ריקים:
df = df[~df['clean_content'].str.strip().eq('')]
df['text_length'] = df['clean_content'].apply(len)
df = df[df['text_length'] > 100]


# כמה שורות?
print(f"✅ מספר שורות בדאטה: {len(df)}")

# אורך טקסטים
# לסינון טקסטים קצרים מדי/ארוכים מדי, ולניתוח.
df['text_length'] = df['clean_content'].apply(len)
print("📊 סטטיסטיקות אורך הטקסטים:")
print(df['text_length'].describe())

# דוגמאות
#מאפשר לראות האם הניקוי עבד, והאם יש טקסטים בעייתיים.


for idx, row in df.sample(3).iterrows():
    print(f"\n--- טקסט לדוגמה ({row['text_length']} תווים) ---")
    print(row['clean_content'][:1000], "...")

# אחוז פסקאות (newlines)
#שימושי לדעת אם הטקסטים מכילים מבנה של פסקאות או שהם שורה אחת ארוכה".
has_newlines = df['clean_content'].apply(lambda x: '\n' in x).mean()
print(f"\n📄 אחוז טקסטים עם פסקאות (\\n): {has_newlines*100:.1f}%")

#נותן אינדיקציה כמה מהטקסטים רלוונטיים לנושא מסוים.
keywords = df['clean_content'].str.contains(r'בגרות|85|תנאי קבלה', case=False, na=False).mean()
print(f"📄 אחוז טקסטים עם מילות מפתח רלוונטיות: {keywords*100:.1f}%")

import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize
import nltk
nltk.download('punkt_tab')

def smart_chunk(text, max_chars=384):
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = ""
    for sent in sentences:
        if len(current_chunk) + len(sent) <= max_chars:
            current_chunk += " " + sent
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sent
    if len(current_chunk.strip()) > 50:
     chunks.append(current_chunk.strip())


    return chunks

chunks = []
for idx, row in df.iterrows():
    for i, chunk in enumerate(smart_chunk(row['clean_content'])):
        chunks.append({
            'id': f"{idx}_{i}",
            'text': chunk,
            #מוסיפה פה שידרוג
            'doc_id': idx

        })

chunks_df = pd.DataFrame(chunks)
print(f"✅ נוצרו {len(chunks_df)} מקטעים.")
chunks_df.head()

from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
#מודל שמתאים גם לעברית
embed_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device='cuda')

embeddings = []
batch_size = 16
texts = chunks_df['text'].tolist()

for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    emb = embed_model.encode(batch, show_progress_bar=True)
    embeddings.append(emb)

embeddings = np.vstack(embeddings).astype("float32")
print(f"✅ סיימנו, embeddings shape: {embeddings.shape}")

#  לוודא שאין אפסים
print(np.isnan(embeddings).sum())
print(np.linalg.norm(embeddings, axis=1).min())


faiss.normalize_L2(embeddings)

index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings)

print(f"✅ אינדקס מוכן עם {len(embeddings)} מקטעים.")

print(chunks_df['text'].value_counts().head(10))

# 🔷 שמור embeddings
np.save('/content/drive/MyDrive/embeddings.npy', embeddings)
print("✅ embeddings נשמרו בדרייב")

# 🔷 שמור FAISS index
faiss.write_index(index, '/content/drive/MyDrive/faiss_index.index')
print("✅ אינדקס FAISS נשמר בדרייב")

# 🔷 שמור את chunks_df כ־CSV
chunks_df.to_csv('/content/drive/MyDrive/chunks_df.csv', index=False)
print("✅ chunks_df נשמר בדרייב")

def retrieve(query, k=5):
    query_emb = embed_model.encode([query]).astype("float32")
    faiss.normalize_L2(query_emb)
    D, I = index.search(query_emb, k)
    return I[0], D[0]

question = "What are the admission requirements?"
true_keywords = ["High School Diploma", "SAT", "Personal Interview"]

ids, scores = retrieve(question)

print("\n🔷 פסקאות שנשלפו:")
for idx, score in zip(ids, scores):
    text = chunks_df.iloc[idx]['text']
    print(f"\n--- (score: {score:.3f}) ---\n{text}")
    if any(kw.lower() in text.lower() for kw in true_keywords):
        print("✅ פסקה רלוונטית נמצאה כאן!")

import pandas as pd

# 🔷 שאלות ותשובות נכונות שאני מצפה
questions = [
    {
        "question": "What are the admission requirements?",
        "expected_keywords": ["High School Diploma", "SAT", "Personal Interview"]
    },
    {
        "question": "How much is tuition?",
        "expected_keywords": ["$3,800", "tuition"]
    },
    {
        "question": "When does the semester start?",
        "expected_keywords": ["September", "school year", "first semester"]
    },
]

results = []

# 🔷 הפונקציה שלנו לאחזור
def retrieve(query, k=5):
    query_clean = re.sub(r'[^\w\s]', '', query.lower())

    query_emb = embed_model.encode([query_clean]).astype("float32")
    faiss.normalize_L2(query_emb)
    D, I = index.search(query_emb, k)
    seen_texts = set()
    unique_indices = []
    unique_scores = []

    for idx, score in zip(I[0], D[0]):
        text = chunks_df.iloc[idx]['text']
        if text not in seen_texts:
            seen_texts.add(text)
            unique_indices.append(idx)
            unique_scores.append(score)
        if len(unique_indices) == 5:
            break

    return unique_indices, unique_scores


# 🔷 עבור כל שאלה — נבדוק אם המידע הופיע
for q in questions:
    ids, scores = retrieve(q['question'], k=10)

    found = False
    retrieved_texts = []

    for idx, score in zip(ids, scores):
        text = chunks_df.iloc[idx]['text']
        retrieved_texts.append(text)
        if any(kw.lower() in text.lower() for kw in q['expected_keywords']):
            found = True

    results.append({
        "Question": q['question'],
        "Found?": "✅" if found else "❌",
        "Top Match": retrieved_texts[0][:150] + "...",
    })

# 🔷 טבלה יפה
df_results = pd.DataFrame(results)
print(df_results)

# 🔷 דיוק כולל
accuracy = (df_results['Found?'] == "✅").mean()
print(f"\n🎯 Recall@5: {accuracy*100:.1f}%")

!pip install transformers accelerate bitsandbytes

from huggingface_hub import login
login()  # ← תדביקו את הטוקן שלכם כאן כקלט

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from transformers import BitsAndBytesConfig

model_id = "mistralai/Mistral-7B-Instruct-v0.2"

# Configure 4-bit quantization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # or "fp4"
    bnb_4bit_compute_dtype=torch.bfloat16,  # or torch.float16
    # bnb_4bit_use_double_quant=False, # optional
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    # torch_dtype="auto" # This might cause issues with 4-bit quantization
    quantization_config=bnb_config
)

qa_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto"
)

def make_short_context(texts, max_chars=3500):
    context = ""
    for t in texts:
        if len(context) + len(t) > max_chars:
            break
        context += t.strip() + "\n"
    return context

def generate_answer(question, context, max_new_tokens=150):
    prompt = f"[INST] Context:\n{context}\n\nQuestion: {question}\nAnswer: [/INST]"
    output = qa_pipeline(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0]['generated_text']


    # חיתוך לפי תגיות INSTRUCT
    if "[/INST]" in output:
        return output.split("[/INST]")[-1].strip()
    else:
        return output.strip()

results = []

for idx, q in enumerate(questions):
    print(f"Processing question {idx+1}/{len(questions)}: {q['question'][:30]}...")  # הדפס התקדמות

    question = q["question"]
    expected_keywords = q["expected_keywords"]

    # אחזור פסקאות רלוונטיות
    ids, _ = retrieve(question)
    top_chunks = [chunks_df.iloc[i]['text'] for i in ids]
    context = make_short_context(top_chunks)

    # הפקת תשובה
    answer = generate_answer(question, context)

    # בדיקת מילות מפתח
    found_keywords = [kw for kw in expected_keywords if kw.lower() in answer.lower()]
    found = "✅" if found_keywords else "❌"

    results.append({
        "Question": question,
        "Answer": answer,
        "Matched Keywords": ", ".join(found_keywords),
        "Success": found
    })

import pandas as pd

df_results = pd.DataFrame(results)
pd.set_option('display.max_colwidth', 200)
display(df_results)

accuracy = (df_results['Success'] == "✅").mean()
print(f"\n🎯 דיוק כולל (Answer Accuracy): {accuracy*100:.1f}%")

# שאלה לדוגמה
question = "What are the admission requirements?"

# נשלוף את הקונטקסט מהאחזור הקודם
ids, scores = retrieve(question)
context = "\n".join(chunks_df.iloc[i]['text'] for i in ids)

# יוצרים תשובה
generated = generate_answer(question, context)

print("🔷 תשובה שנוצרה:")
print(generated)

!pip install -U bitsandbytes
